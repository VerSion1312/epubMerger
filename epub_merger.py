#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EPUBæ–‡ä»¶åˆå¹¶å·¥å…·
å°†å¤šä¸ªEPUBæ–‡ä»¶æŒ‰é¡ºåºåˆå¹¶æˆä¸€ä¸ªæ–‡ä»¶ï¼Œä¿ç•™åŸæœ‰çš„å›¾ç‰‡å’Œæ–‡å­—ï¼Œä¿æŒåŸæœ‰çš„é¡ºåºå’Œä½ç½®
"""

import os
import zipfile
import xml.etree.ElementTree as ET
from pathlib import Path
import shutil
import tempfile
from typing import List, Dict, Tuple
import argparse
import logging
import re
import urllib.parse

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EpubMerger:
    def __init__(self, language='zh-CN'):
        self.namespace = {'ns': 'http://www.idpf.org/2007/opf'}
        self.merged_content = []
        self.merged_resources = {}
        self.resource_counter = 1
        # æ·»åŠ èµ„æºè·¯å¾„æ˜ å°„
        self.resource_mapping = {}  # åŸå§‹è·¯å¾„ -> æ–°è·¯å¾„
        self.id_mapping = {}  # åŸå§‹ID -> æ–°ID
        # æ·»åŠ è¯­è¨€è®¾ç½®
        self.language = language
        # æ·»åŠ æ–‡ä»¶åè®¡æ•°å™¨ï¼Œé¿å…é‡å
        self.filename_counter = {}
        
    def extract_epub(self, epub_path: str) -> str:
        """è§£å‹EPUBæ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•"""
        temp_dir = tempfile.mkdtemp()
        with zipfile.ZipFile(epub_path, 'r') as zip_ref:
            zip_ref.extractall(temp_dir)
        return temp_dir
    
    def parse_container_xml(self, temp_dir: str) -> str:
        """è§£æcontainer.xmlè·å–content.opfè·¯å¾„"""
        container_path = os.path.join(temp_dir, 'META-INF', 'container.xml')
        if not os.path.exists(container_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°container.xmlæ–‡ä»¶: {container_path}")
        
        tree = ET.parse(container_path)
        root = tree.getroot()
        
        # å®šä¹‰container.xmlçš„å‘½åç©ºé—´
        container_ns = {'container': 'urn:oasis:names:tc:opendocument:xmlns:container'}
        
        # æŸ¥æ‰¾rootfileå…ƒç´ ï¼Œå°è¯•ä¸åŒçš„æ–¹æ³•
        rootfiles = root.findall('.//rootfile')
        if not rootfiles:
            # å°è¯•å¸¦å‘½åç©ºé—´çš„æ–¹æ³•
            rootfiles = root.findall('.//container:rootfile', container_ns)
        
        for rootfile in rootfiles:
            full_path = rootfile.get('full-path')
            if full_path:
                return os.path.join(temp_dir, full_path)
        
        # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾
        for elem in root.iter():
            if elem.tag.endswith('rootfile') or 'rootfile' in elem.tag:
                full_path = elem.get('full-path')
                if full_path:
                    return os.path.join(temp_dir, full_path)
        
        raise ValueError("åœ¨container.xmlä¸­æ‰¾ä¸åˆ°rootfile")
    
    def parse_content_opf(self, opf_path: str) -> Tuple[List[str], Dict[str, str]]:
        """è§£æcontent.opfæ–‡ä»¶ï¼Œè·å–spineé¡ºåºå’Œmanifestèµ„æº"""
        tree = ET.parse(opf_path)
        root = tree.getroot()
        
        # è·å–manifestä¸­çš„èµ„æº
        manifest = {}
        for item in root.findall('.//ns:item', self.namespace):
            item_id = item.get('id')
            href = item.get('href')
            media_type = item.get('media-type')
            if item_id and href:
                manifest[item_id] = {
                    'href': href,
                    'media-type': media_type
                }
        
        # è·å–spineé¡ºåº
        spine = []
        for itemref in root.findall('.//ns:itemref', self.namespace):
            idref = itemref.get('idref')
            if idref in manifest:
                spine.append(idref)
        
        return spine, manifest
    
    def read_file_content(self, base_path: str, file_path: str) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        full_path = os.path.join(base_path, file_path)
        if os.path.exists(full_path):
            with open(full_path, 'r', encoding='utf-8') as f:
                return f.read()
        return ""
    
    def copy_resource(self, source_base: str, source_path: str, target_base: str, target_path: str):
        """å¤åˆ¶èµ„æºæ–‡ä»¶"""
        source_full = os.path.join(source_base, source_path)
        target_full = os.path.join(target_base, target_path)
        
        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(target_full), exist_ok=True)
        
        if os.path.exists(source_full):
            shutil.copy2(source_full, target_full)
            logger.info(f"å¤åˆ¶èµ„æº: {source_path} -> {target_path}")
    
    def normalize_path(self, path: str, base_path: str) -> str:
        """æ ‡å‡†åŒ–è·¯å¾„ï¼Œå¤„ç†å„ç§ç›¸å¯¹è·¯å¾„æƒ…å†µ"""
        if not path:
            return path
            
        original_path = path
        
        # ç§»é™¤å¼€å¤´çš„ './'
        if path.startswith('./'):
            path = path[2:]
        
        # å¤„ç†ç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºEPUBæ ¹ç›®å½•ï¼‰
        if path.startswith('/'):
            path = path[1:]
        
        # å¤„ç†åŒ…å« '../' çš„è·¯å¾„
        if '../' in path:
            # è®¡ç®—ç›¸å¯¹è·¯å¾„
            path_parts = path.split('/')
            base_parts = base_path.split(os.sep)
            
            # ç§»é™¤è·¯å¾„ä¸­çš„ '..'
            while path_parts and path_parts[0] == '..':
                path_parts.pop(0)
                if base_parts:
                    base_parts.pop()
            
            # é‡æ–°ç»„åˆè·¯å¾„
            if base_parts:
                path = os.path.join(*base_parts, *path_parts)
            else:
                path = '/'.join(path_parts)
        
        # ç»Ÿä¸€è·¯å¾„åˆ†éš”ç¬¦ä¸º '/'
        path = path.replace('\\', '/')
        
        logger.debug(f"è·¯å¾„æ ‡å‡†åŒ–: {original_path} -> {path}")
        return path
    
    def get_unique_filename(self, original_filename: str) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œé¿å…é‡åå†²çª"""
        name, ext = os.path.splitext(original_filename)
        if original_filename not in self.filename_counter:
            self.filename_counter[original_filename] = 1
            return original_filename
        else:
            self.filename_counter[original_filename] += 1
            return f"{name}_{self.filename_counter[original_filename]}{ext}"
    
    def update_html_references(self, html_content: str, base_path: str, resource_mapping: Dict) -> str:
        """æ›´æ–°HTMLæ–‡ä»¶ä¸­çš„èµ„æºå¼•ç”¨"""
        if not html_content:
            return html_content
            
        # æ›´æ–°imgæ ‡ç­¾çš„srcå±æ€§
        def update_img_src(match):
            src = match.group(1)
            if src and not src.startswith(('http://', 'https://', 'data:')):
                # å¤„ç†ç›¸å¯¹è·¯å¾„
                if src.startswith('#'):
                    return match.group(0)  # é”šç‚¹é“¾æ¥ï¼Œä¿æŒä¸å˜
                
                # è§£æç›¸å¯¹è·¯å¾„
                try:
                    # æ ‡å‡†åŒ–è·¯å¾„
                    normalized_path = self.normalize_path(src, base_path)
                    
                    # æŸ¥æ‰¾å¯¹åº”çš„æ–°è·¯å¾„
                    if normalized_path in resource_mapping:
                        new_path = resource_mapping[normalized_path]
                        logger.info(f"æ›´æ–°å›¾ç‰‡å¼•ç”¨: {src} -> {new_path}")
                        return f'src="{new_path}"'
                    else:
                        # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„å˜ä½“
                        possible_paths = [
                            normalized_path,
                            src,
                            src.lstrip('./'),
                            src.lstrip('/'),
                            os.path.basename(src)
                        ]
                        
                        for test_path in possible_paths:
                            if test_path in resource_mapping:
                                new_path = resource_mapping[test_path]
                                logger.info(f"æ›´æ–°å›¾ç‰‡å¼•ç”¨(å˜ä½“): {src} -> {new_path}")
                                return f'src="{new_path}"'
                        
                        logger.warning(f"æœªæ‰¾åˆ°èµ„æºæ˜ å°„: {normalized_path} (åŸå§‹: {src})")
                        logger.debug(f"å¯ç”¨çš„èµ„æºæ˜ å°„é”®: {list(resource_mapping.keys())[:10]}...")
                except Exception as e:
                    logger.warning(f"æ›´æ–°å›¾ç‰‡å¼•ç”¨å¤±è´¥: {src}, é”™è¯¯: {e}")
            
            return match.group(0)
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›´æ–°imgæ ‡ç­¾
        html_content = re.sub(r'src=["\']([^"\']*)["\']', update_img_src, html_content, flags=re.IGNORECASE)
        
        # æ›´æ–°CSSä¸­çš„èƒŒæ™¯å›¾ç‰‡å¼•ç”¨
        def update_css_background(match):
            url = match.group(1)
            if url and not url.startswith(('http://', 'https://', 'data:')):
                # ç§»é™¤url()åŒ…è£…
                clean_url = url.strip('"\'')
                if clean_url.startswith('#'):
                    return match.group(0)  # é”šç‚¹é“¾æ¥ï¼Œä¿æŒä¸å˜
                
                try:
                    # æ ‡å‡†åŒ–è·¯å¾„
                    normalized_path = self.normalize_path(clean_url, base_path)
                    
                    if normalized_path in resource_mapping:
                        new_path = resource_mapping[normalized_path]
                        logger.info(f"æ›´æ–°CSSèƒŒæ™¯å›¾ç‰‡: {clean_url} -> {new_path}")
                        return f'url("{new_path}")'
                    else:
                        # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„å˜ä½“
                        possible_paths = [
                            normalized_path,
                            clean_url,
                            clean_url.lstrip('./'),
                            clean_url.lstrip('/'),
                            os.path.basename(clean_url)
                        ]
                        
                        for test_path in possible_paths:
                            if test_path in resource_mapping:
                                new_path = resource_mapping[test_path]
                                logger.info(f"æ›´æ–°CSSèƒŒæ™¯å›¾ç‰‡(å˜ä½“): {clean_url} -> {new_path}")
                                return f'url("{new_path}")'
                        
                        logger.warning(f"æœªæ‰¾åˆ°CSSèµ„æºæ˜ å°„: {normalized_path} (åŸå§‹: {clean_url})")
                except Exception as e:
                    logger.warning(f"æ›´æ–°CSSèƒŒæ™¯å›¾ç‰‡å¤±è´¥: {clean_url}, é”™è¯¯: {e}")
            
            return match.group(0)
        
        # æ›´æ–°CSSä¸­çš„url()å¼•ç”¨
        html_content = re.sub(r'url\(["\']?([^"\')\s]*)["\']?\)', update_css_background, html_content, flags=re.IGNORECASE)
        
        return html_content
    
    def merge_epub(self, epub_files: List[str], output_path: str):
        """åˆå¹¶å¤šä¸ªEPUBæ–‡ä»¶"""
        logger.info(f"å¼€å§‹åˆå¹¶ {len(epub_files)} ä¸ªEPUBæ–‡ä»¶")
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºåˆå¹¶
        merged_temp_dir = tempfile.mkdtemp()
        
        try:
            # åˆ›å»ºMETA-INFç›®å½•
            meta_inf_dir = os.path.join(merged_temp_dir, 'META-INF')
            os.makedirs(meta_inf_dir, exist_ok=True)
            
            # åˆ›å»ºcontainer.xml
            container_xml = '''<?xml version="1.0" encoding="UTF-8"?>
<container version="1.0" xmlns="urn:oasis:names:tc:opendocument:xmlns:container">
    <rootfiles>
        <rootfile full-path="content.opf" media-type="application/oebps-package+xml"/>
    </rootfiles>
</container>'''
            
            with open(os.path.join(meta_inf_dir, 'container.xml'), 'w', encoding='utf-8') as f:
                f.write(container_xml)
            
            # åˆå¹¶æ‰€æœ‰EPUBæ–‡ä»¶
            all_spine_items = []
            all_manifest = {}
            all_resources = {}
            
            # é‡ç½®å…¨å±€èµ„æºæ˜ å°„
            self.resource_mapping = {}
            self.id_mapping = {}
            self.filename_counter = {}
            
            for i, epub_file in enumerate(epub_files):
                logger.info(f"å¤„ç†ç¬¬ {i+1} ä¸ªæ–‡ä»¶: {epub_file}")
                
                # è§£å‹EPUB
                temp_dir = self.extract_epub(epub_file)
                
                try:
                    # è§£æcontent.opf
                    opf_path = self.parse_container_xml(temp_dir)
                    spine, manifest = self.parse_content_opf(opf_path)
                    
                    # è·å–åŸºç¡€è·¯å¾„
                    base_path = os.path.dirname(opf_path)
                    
                    # é¦–å…ˆå¤„ç†æ‰€æœ‰èµ„æºï¼ˆå›¾ç‰‡ã€CSSç­‰ï¼‰ï¼Œå»ºç«‹æ˜ å°„å…³ç³»
                    for item_id, item_info in manifest.items():
                        if item_id not in spine:  # ä¸æ˜¯spineé¡¹ç›®
                            href = item_info['href']
                            media_type = item_info['media-type']
                            
                            # ç”Ÿæˆæ–°çš„ID
                            new_id = f"item_{self.resource_counter:04d}"
                            self.resource_counter += 1
                            
                            # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
                            original_filename = os.path.basename(href)
                            unique_filename = self.get_unique_filename(original_filename)
                            new_href = f"resources/{unique_filename}"
                            
                            # å¤åˆ¶æ–‡ä»¶åˆ°resourcesç›®å½•
                            self.copy_resource(base_path, href, merged_temp_dir, new_href)
                            
                            # å»ºç«‹æ˜ å°„å…³ç³»ï¼ˆä½¿ç”¨æ–‡ä»¶å†…çš„ç›¸å¯¹è·¯å¾„ï¼‰
                            self.resource_mapping[href] = new_href
                            self.id_mapping[item_id] = new_id
                            
                            logger.info(f"èµ„æºæ˜ å°„: {href} -> {new_href} (ç±»å‹: {media_type})")
                            
                            all_resources[new_id] = {
                                'href': new_href,
                                'media_type': media_type,
                                'original_href': href
                            }
                    
                    # ç„¶åå¤„ç†spineé¡¹ç›®ï¼ˆHTMLæ–‡ä»¶ï¼‰
                    for item_id in spine:
                        if item_id in manifest:
                            item_info = manifest[item_id]
                            href = item_info['href']
                            media_type = item_info['media-type']
                            
                            # ç”Ÿæˆæ–°çš„ID
                            new_id = f"item_{self.resource_counter:04d}"
                            self.resource_counter += 1
                            
                            # è¯»å–æ–‡ä»¶å†…å®¹
                            content = self.read_file_content(base_path, href)
                            
                            # å¦‚æœæ˜¯HTMLæ–‡ä»¶ï¼Œéœ€è¦æ›´æ–°èµ„æºå¼•ç”¨
                            if media_type == 'application/xhtml+xml':
                                logger.info(f"å¤„ç†HTMLæ–‡ä»¶: {href}")
                                # ä½¿ç”¨å…¨å±€èµ„æºæ˜ å°„
                                content = self.update_html_references(content, base_path, self.resource_mapping)
                            
                            # ä¿å­˜åˆ°åˆå¹¶çš„èµ„æºä¸­
                            all_resources[new_id] = {
                                'content': content,
                                'media_type': media_type,
                                'original_href': href
                            }
                            
                            all_spine_items.append(new_id)
                
                finally:
                    # æ¸…ç†ä¸´æ—¶ç›®å½•
                    shutil.rmtree(temp_dir)
            
            # åˆ›å»ºåˆå¹¶åçš„content.opf
            self.create_merged_opf(merged_temp_dir, all_spine_items, all_resources)
            
            # åˆ›å»ºEPUBæ–‡ä»¶
            self.create_epub(merged_temp_dir, output_path)
            
            logger.info(f"åˆå¹¶å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {output_path}")
            
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            shutil.rmtree(merged_temp_dir)
    
    def create_merged_opf(self, temp_dir: str, spine_items: List[str], resources: Dict):
        """åˆ›å»ºåˆå¹¶åçš„content.opfæ–‡ä»¶"""
        opf_content = f'''<?xml version="1.0" encoding="UTF-8"?>
<package version="3.0" xmlns="http://www.idpf.org/2007/opf" unique-identifier="uid">
    <metadata xmlns:dc="http://purl.org/dc/elements/1.1/">
        <dc:title>åˆå¹¶çš„EPUBæ–‡ä»¶</dc:title>
        <dc:creator>EPUBåˆå¹¶å·¥å…·</dc:creator>
        <dc:language>{self.language}</dc:language>
        <dc:identifier id="uid">merged-epub-{self.resource_counter}</dc:identifier>
    </metadata>
    <manifest>
'''
        
        # æ·»åŠ æ‰€æœ‰èµ„æºåˆ°manifest
        for item_id, item_info in resources.items():
            if 'href' in item_info:
                # æ–‡ä»¶èµ„æº
                opf_content += f'        <item id="{item_id}" href="{item_info["href"]}" media-type="{item_info["media_type"]}"/>\n'
            else:
                # å†…å®¹èµ„æº
                opf_content += f'        <item id="{item_id}" href="{item_id}.xhtml" media-type="{item_info["media_type"]}"/>\n'
        
        opf_content += '''    </manifest>
    <spine>
'''
        
        # æ·»åŠ spineé¡¹ç›®
        for item_id in spine_items:
            opf_content += f'        <itemref idref="{item_id}"/>\n'
        
        opf_content += '''    </spine>
</package>'''
        
        # å†™å…¥content.opf
        with open(os.path.join(temp_dir, 'content.opf'), 'w', encoding='utf-8') as f:
            f.write(opf_content)
        
        # å†™å…¥æ‰€æœ‰å†…å®¹æ–‡ä»¶
        for item_id, item_info in resources.items():
            if 'content' in item_info:
                content_file = os.path.join(temp_dir, f'{item_id}.xhtml')
                with open(content_file, 'w', encoding='utf-8') as f:
                    f.write(item_info['content'])
    
    def create_epub(self, temp_dir: str, output_path: str):
        """åˆ›å»ºæœ€ç»ˆçš„EPUBæ–‡ä»¶"""
        with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(temp_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, temp_dir)
                    zipf.write(file_path, arcname)

def main():
    parser = argparse.ArgumentParser(description='åˆå¹¶å¤šä¸ªEPUBæ–‡ä»¶')
    parser.add_argument('input_files', nargs='+', help='è¾“å…¥çš„EPUBæ–‡ä»¶åˆ—è¡¨')
    parser.add_argument('-o', '--output', default='merged.epub', help='è¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('-l', '--language', default='zh-CN', 
                       help='è¾“å‡ºEPUBçš„è¯­è¨€ä»£ç  (é»˜è®¤: zh-CN, ä¾‹å¦‚: en-US, ja-JP, ko-KR)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    for epub_file in args.input_files:
        if not os.path.exists(epub_file):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {epub_file}")
            return
    
    # åˆ›å»ºåˆå¹¶å™¨å¹¶æ‰§è¡Œåˆå¹¶
    merger = EpubMerger(language=args.language)
    try:
        merger.merge_epub(args.input_files, args.output)
        print(f"âœ… åˆå¹¶æˆåŠŸï¼è¾“å‡ºæ–‡ä»¶: {args.output}")
        print(f"ğŸŒ è¯­è¨€è®¾ç½®: {args.language}")
    except Exception as e:
        logger.error(f"åˆå¹¶å¤±è´¥: {str(e)}")
        print(f"âŒ åˆå¹¶å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    main() 